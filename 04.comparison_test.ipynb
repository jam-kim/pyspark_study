{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 환경\n",
    "\n",
    "CPU: i7-8550U / RAM: 16GB\n",
    "\n",
    "jupyterlab==2.0.1 / pyspark==2.4.5 / pandas==1.0.1 / numpy==1.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_check(code, nums=10):\n",
    "    output = dict()\n",
    "    \n",
    "    l = list()\n",
    "    \n",
    "    for num in range(nums):\n",
    "        start_time = time.time()\n",
    "        exec(code)\n",
    "        end_time = time.time()\n",
    "\n",
    "        time_diff = round(end_time - start_time,3)  \n",
    "        l.append(time_diff)\n",
    "        print(num+1,':',time_diff)\n",
    "        \n",
    "    output['avg'] = round(sum(l) / len(l), 3)\n",
    "    output['max'] = max(l)\n",
    "    output['min'] = min(l)\n",
    "    output['nums'] = nums\n",
    "    \n",
    "    return output \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10회 평균 값\n",
    "- 단위: 초\n",
    "\n",
    "## Read csv\n",
    "\n",
    "\n",
    "| row        | pyspark | pandas  | numpy   |   \n",
    "|------------|:-------:|:-------:|:-------:| \n",
    "| 1,000      | 0.108   | 0.01    | 0.024   | \n",
    "| 10,000     | 0.11    | 0.049   | 0.213   |  \n",
    "| 100,000    | 0.114   | 0.463   | 2.279   | \n",
    "| 500,000    | 0.111   | 2.221   | 12.927  | \n",
    "| 1,000,000  | 0.135   | 5.563   | 27.054  | \n",
    "| 5,000,000  | 0.124   | 26.565  | 252.865 | \n",
    "| 10,000,000 | 0.119   | 59.202  | 뻗음    | \n",
    "  \n",
    "사용 코드\n",
    "- pyspark: ```spark.read.csv('../path/data.csv', header=True)```\n",
    "- pandas: ```pd.read_csv('../path/data.csv')```\n",
    "- numpy: ```np.genfromtxt('../path/data.csv', delimiter=',', names=True)```\n",
    "\n",
    "---\n",
    "\n",
    "## where\n",
    "\n",
    "| row        | pyspark | pandas  |   \n",
    "|------------|:-------:|:-------:|\n",
    "| 1,000      | 0.006   | 0.002   | \n",
    "| 10,000     | 0.004   | 0.002   |  \n",
    "| 100,000    | 0.004   | 0.005   | \n",
    "| 500,000    | 0.003   | 0.015   | \n",
    "| 1,000,000  | 0.004   | 0.036   | \n",
    "| 5,000,000  | 0.006   | 0.191   | \n",
    "| 10,000,000 | 0.006   | 0.854   | \n",
    "\n",
    "사용 코드\n",
    "- pyspark: ```spark_df.filter('condition')```\n",
    "- pandas: ```pandas_df[(condition)]```\n",
    "\n",
    "---\n",
    "\n",
    "## group by\n",
    "\n",
    "| row        | pyspark | pandas  |   \n",
    "|------------|:-------:|:-------:|\n",
    "| 1,000      | 0.01    | 0.001   | \n",
    "| 10,000     | 0.009   | 0.003   |  \n",
    "| 100,000    | 0.01    | 0.022   | \n",
    "| 500,000    | 0.01    | 0.16    | \n",
    "| 1,000,000  | 0.013   | 0.199   | \n",
    "| 5,000,000  | 0.009   | 0.61    | \n",
    "| 10,000,000 | 0.01    | 1.209   | \n",
    "\n",
    "사용 코드\n",
    "- pyspark: ```spark_df.groupBy('').avg('')```\n",
    "- pandas: ```pandas_df.groupby([''])[''].mean()```\n",
    "\n",
    "---\n",
    "\n",
    "## order by\n",
    "\n",
    "| row        | pyspark | pandas  |   \n",
    "|------------|:-------:|:-------:|\n",
    "| 1,000      | 0.006   | 0.004   | \n",
    "| 10,000     | 0.005   | 0.005   |  \n",
    "| 100,000    | 0.006   | 0.037   | \n",
    "| 500,000    | 0.006   | 0.189   | \n",
    "| 1,000,000  | 0.006   | 0.559   | \n",
    "| 5,000,000  | 0.005   | 4.062   | \n",
    "| 10,000,000 | 0.005   | 9.43    | \n",
    "\n",
    "사용 코드\n",
    "- pyspark: ```spark_df.sort('')```\n",
    "- pandas: ```pandas_df.sort_values('')```\n",
    "\n",
    "---\n",
    "\n",
    "## delete\n",
    "- 특정 Column 삭제를 의미합니다\n",
    "\n",
    "| row        | pyspark | pandas  |   \n",
    "|------------|:-------:|:-------:|\n",
    "| 1,000      | 0.003   | 0.001   | \n",
    "| 10,000     | 0.003   | 0.002   |  \n",
    "| 100,000    | 0.002   | 0.012   | \n",
    "| 500,000    | 0.003   | 0.06    | \n",
    "| 1,000,000  | 0.003   | 0.118   | \n",
    "| 5,000,000  | 0.002   | 0.604   | \n",
    "| 10,000,000 | 0.002   | 1.198   | \n",
    "\n",
    "사용 코드\n",
    "- pyspark: ```spark_df.drop('')```\n",
    "- pandas: ```pandas_df.drop(columns='')```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
